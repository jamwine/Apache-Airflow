{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588482df",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Apache Airflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3713d55",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "* https://airflow.apache.org/\n",
    "* https://livebook.manning.com/book/data-pipelines-with-apache-airflow/chapter-1/v-5/\n",
    "\n",
    "**Orchestrator** or **Workflow manager** allows us to create **Data Pipelines** & describe all steps of our Data Flow: from **where** to where, **what**, **when** and **how** - multiple task in any sequence (not only classical **ETL**).\n",
    "\n",
    "**Apache Airflow** is one of the most popular workflow management systems to manage data pipelines. It is a platform to programmatically author, schedule and monitor workflows as **directed acyclic graphs** (**DAGs**) of tasks. The airflow scheduler executes our tasks on an array of workers while following the specified dependencies. **Workflows** are defined as code, thus they become more maintainable, versionable, testable, and collaborative.\n",
    "* Airflow command line utilities make performing complex surgeries on DAGs a snap.\n",
    "* Airflow user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462ed6e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Directed Acyclic Graphs (DAGs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769889f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**DAGs** are a special subset of graphs in which the **edges** between **nodes** have a specific direction, and no **cycles** exist. When we say `no cycles exist` what we mean is the nodes cant create a path back to themselves.\n",
    "\n",
    "<img src='imgs/DAG.png' alt='DAG' width=40%>\n",
    "\n",
    "### Nodes\n",
    "A step or task in the data pipeline process.\n",
    "\n",
    "### Edges\n",
    "The dependencies or relationships other between nodes.\n",
    "\n",
    "<img src='imgs/dag_pipeline.png' alt='dag_pipeline' width=60%>\n",
    "\n",
    "\n",
    "A DAG is a collection of Tasks that are ordered to reflect the functionality, requirements and dependencies of the workflow.\n",
    "\n",
    "\n",
    "#### Are there real world cases where a data pipeline is not DAG?\n",
    "\n",
    "It is possible to model a data pipeline that is not a DAG, meaning that it contains a cycle within the process. However, the vast majority of use cases for data pipelines can be described as a directed acyclic graph (DAG). This makes the code more understandable and maintainable.\n",
    "\n",
    "#### Can we have two different pipelines for the same data and can we merge them back together?\n",
    "\n",
    "Yes. It's common for a data pipeline to take the same dataset, perform two different processes to analyze the it, then merge the results of those two processes back together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d40a02",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Components of Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2479a6d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='imgs/airflow_main_components.png' alt='airflow_main_components' width=50%>\n",
    "\n",
    "* **Scheduler** orchestrates the execution of jobs on a trigger or schedule interval. The Scheduler chooses how to prioritize the running and execution of tasks within the system.\n",
    "\n",
    "\n",
    "* **Executor**, also known as **Work Queue** is used by the scheduler in most Airflow installations to deliver tasks that need to be run to the Workers. Executors are the `workstation for tasks` and acts as a middle man to handle resource allocation and distribute task completion. There are many options available in Airflow for executors:\n",
    "    * Sequential Executor\n",
    "    * Debug Executor\n",
    "    * Local Executor(Single Node Arch)\n",
    "    * Dask Executor\n",
    "    * Celery Executor\n",
    "    * Kubernetes Executor\n",
    "    * Scaling Out with Mesos (community contributed)\n",
    "    \n",
    "    \n",
    "* **Worker** processes execute the operations defined in each DAG. In most Airflow installations, workers pull from the work queue when it is ready to process a task. When the worker completes the execution of the task, it will attempt to process more work from the work queue until there is no further work remaining. When work in the queue arrives, the worker will begin to process it.\n",
    "\n",
    "\n",
    "* **Database** saves credentials, connections, history, and configuration. The database, often referred to as the **metadata database**, also stores the state of all tasks in the system. Airflow components interact with the database with the Python ORM, SQLAlchemy.\n",
    "\n",
    "\n",
    "* **Web Interface** provides a control dashboard for users and maintainers. The web interface allows users to perform tasks such as stopping and starting DAGs, retrying failed tasks, configuring credentials, The web interface visualizes the DAGs parsed by the scheduler and is built using the Flask web-development microframework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f190016",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Order of Operations for an Airflow DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04d2cf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='imgs/af_working.png' alt='af_working' width=55%>\n",
    "\n",
    "* The Airflow Scheduler starts DAGs based on time or external triggers.\n",
    "\n",
    "* Once a DAG is started, the Scheduler looks at the steps within the DAG and determines which steps can run by looking at their dependencies.\n",
    "\n",
    "* The Scheduler places runnable steps in the queue.\n",
    "\n",
    "* Workers pick up those tasks and run them. \n",
    "\n",
    "* Tasks get transitioned from one state to another when DAG is run. The below diagram explains the transition:\n",
    "\n",
    "<img src='imgs/transition.png' alt='transition' width=60%>\n",
    "\n",
    "* Once the worker has finished running the step, the final status of the task is recorded and additional tasks are placed by the scheduler until all tasks are complete.\n",
    "\n",
    "* Once all tasks have been completed, the DAG is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbe8d8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Building a Data Pipeline in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee70c4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A DAG.py file is created in the DAG folder in Airflow, containing the imports for operators, DAG configurations like schedule and DAG name, and defining the dependency and sequence of tasks.\n",
    "\n",
    "<img src='imgs/operator.png' alt='operator' width=20%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9a37a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf916e7f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Operators are created in the Operator folder in Airflow. They contain Python Classes that have logic to perform tasks. They are called in the DAG.py file.\n",
    "\n",
    "\n",
    "* **Operators** define the atomic steps of work that make up a DAG. Instantiated operators are referred to as **Tasks**. Airflow comes with many Operators that can perform common operations:\n",
    "    * PythonOperator - calls a python function\n",
    "    * PostgresOperator - - executes a SQL command\n",
    "    * RedshiftToS3Operator\n",
    "    * S3ToRedshiftOperator\n",
    "    * BashOperator - executes a UNIX command\n",
    "    * SimpleHttpOperator\n",
    "    * Sensor - waits for a certain time, file, database row, S3 key, etc.\n",
    "    * EmailOperator - sends an email\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e56e2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Schedules "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39bb44",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* `schedule_interval` tells when to run this dag. **Schedules** are optional, and may be defined with **cron strings** or **Airflow Presets**. Airflow provides the following presets:\n",
    "    * @once - Run a DAG once and then never again\n",
    "    * @hourly - Run the DAG every hour\n",
    "    * @daily - Run the DAG every day\n",
    "    * @weekly - Run the DAG every week\n",
    "    * @monthly - Run the DAG every month\n",
    "    * @yearly- Run the DAG every year\n",
    "    * None - Only run the DAG when the user initiates it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b7a0af",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Parameters/Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ffe284",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* While creating a DAG, give it a **name**, a **description**, a **start date**, and an **interval**.\n",
    "    \n",
    "    \n",
    "* `start_date`: If start date is in the past, Airflow will run DAG as many times as there are schedule intervals between that start date and the current date.\n",
    "\n",
    "\n",
    "* `end_date`: Unless we specify an optional end date, Airflow will continue to run our DAGs until we disable or delete the DAG.\n",
    "\n",
    "\n",
    "* `max_active_run` tells how many instances of the Dag can run concurrently.\n",
    "\n",
    "\n",
    "* `retries` argument re-run a failed task multiple times before aborting the workflow run.\n",
    "\n",
    "\n",
    "* `on_success_callback` and `on_failure_callback` arguments are used to trigger some actions once the workflow succeeds or fails respectively. This will be useful to send personalized alerts to internal team via Slack, Email, or any other API call when a workflow task succeeds or fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d966a18",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### First DAG (Basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f5ce4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Initializing the default arguments that we'll pass to our DAG\n",
    "default_args = {\n",
    "    'owner': 'jamwine',\n",
    "    'start_date': datetime(2021, 1, 1),\n",
    "    'retries': 1, \n",
    "    'on_failure_callback': task_failure_alert,\n",
    "    'max_active_runs': 3\n",
    "}\n",
    "\n",
    "def message():\n",
    "    logging.info(\"Airflow is awesome!\")\n",
    "    logging.info(\"Airflow uses DAGs...\")\n",
    "\n",
    "# Creating a DAG instance\n",
    "dag1 = DAG(\n",
    "        dag_id=\"dag1\",\n",
    "        description=\"Test DAG 1\",\n",
    "        default_args=default_args,\n",
    "        start_date=datetime.now(),\n",
    "        schedule_interval=\"@hourly\"\n",
    ")\n",
    "\n",
    "message_task = PythonOperator(\n",
    "    task_id=\"message_task\",\n",
    "    python_callable=message,\n",
    "    dag=dag1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fa9d0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Task Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28dd55",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Task Dependencies** can be described programmatically in Airflow using `>>` and `<<`\n",
    "* `a >> b` means **a comes before b**\n",
    "* `a << b` means **a comes after b**\n",
    "\n",
    "Tasks dependencies can also be set with `set_downstream` and `set_upstream`.\n",
    "* `a.set_downstream(b)` means **a comes before b**\n",
    "* `a.set_upstream(b)` means **a comes after b**\n",
    "\n",
    "Parallel tasks are included in `[]`. For example, `[a,b] >> c`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566ca1fb",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005d76d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **Variables** are defined as a generic way to store and retrieve arbitrary content within Airflow. They are represented as a simple key value stored into the meta database of Airflow.\n",
    "\n",
    "\n",
    "*  Variables are useful for storing and retrieving data at runtime, in avoiding hard-coding values, and from code repetitions within our DAGs.\n",
    "\n",
    "\n",
    "* Airflow goes through **2-layers** before reaching the metastore. If the variable is found in one of these two layers, Airflow doesnâ€™t need to create a connection, thus it is better optimized.\n",
    "\n",
    "\n",
    "*  Command Line Interface and Environment Variables Reference: https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfee978",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### XComs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7f5b4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **XComs** or **Cross Communication messages** is designed to communicate small amount of data between tasks. Basically, they let tasks exchange messages within our DAG. For example, if task_B depends on task_A, task_A can push data into a XCom and task_B can pull this data to use it. \n",
    "\n",
    "\n",
    "* We use `xcom_push` and `xcom_pull` to push and retrieve variables.\n",
    "\n",
    "\n",
    "* These methods `xcom_push` and `xcom_pull` are only accessible from a task instance object. With the PythonOperator, we can access them by passing the parameter `ti` to the python callable function. \n",
    "\n",
    "\n",
    "* We can pull XComs from multiple tasks at once.\n",
    "\n",
    "\n",
    "* In the BashOperator, `do_xcom_push` allows us to push the **last line** written to stdout into a XCom. By default, `do_xcom_push` is set to **True**.\n",
    "\n",
    "\n",
    "*  By default, when a XCom is automatically created by returning a value, Airflow assigns the key `return_value`. The key `return_value` indicates that this XCom has been created by returning the value from the operator. The XCom values gets stored into the `metadata database` of Airflow with the key `return_value`.\n",
    "\n",
    "<img src='imgs/af_variable.png' alt='af_variable' width=40%>\n",
    "\n",
    "* XComs create implicit dependencies between the tasks that are not visible from the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a69b9f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Macros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eba2d0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **Jinja templates** and **Macros** in Apache Airflow are the way to pass dynamic data to our DAGs at runtime. \n",
    "\n",
    "\n",
    "* Templating allows us to interpolate values at run time in static files such as HTML or SQL files, by placing special placeholders in them indicating where the values should be and/or how they should be displayed.\n",
    "\n",
    "\n",
    "* The curly brackets `{{ }}` represent **placeholders** in which the value is replaced at the **runtime** each time the DAG gets triggered. For example, `{{ ds }}` is a macro that gets replaced by the **execution date** of the DAG.\n",
    "\n",
    "\n",
    "* **Macros** are functions that take an input, modify that input and give the modified output. Macros can be used in our templates by calling them with the following notation: `macro.macro_func()`.\n",
    "\n",
    "\n",
    "* Apache Airflow brings predefined variables that we can use in our templates. They are very useful since they allow us to have information about the current executing DAG and task. Macros Reference: https://airflow.apache.org/docs/apache-airflow/stable/macros-ref.html\n",
    "\n",
    "\n",
    "*  We can also check our Jinja Templates rendered before even executing our DAG in the Rendered section of Airflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0c8c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
