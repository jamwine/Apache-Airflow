{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588482df",
   "metadata": {},
   "source": [
    "# Apache Airflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3713d55",
   "metadata": {},
   "source": [
    "* https://airflow.apache.org/\n",
    "\n",
    "**Orchestrator** or **Workflow manager** allows us to create **Data Pipelines** & describe all steps of our Data Flow: from **where** to where, **what**, **when** and **how** - multiple task in any sequence (not only classical **ETL**).\n",
    "\n",
    "**Apache Airflow** is one of the most popular open-source workflow management systems to manage data pipelines. It is a platform to programmatically author, schedule and monitor workflows as **directed acyclic graphs** (**DAGs**) of tasks. The airflow scheduler executes our tasks on an array of workers while following the specified dependencies. A **workflow** can be defined as any sequence of steps taken to accomplish a particular goal, thus they become more maintainable, versionable, testable, and collaborative.\n",
    "* Airflow command line utilities make performing complex surgeries on DAGs a snap.\n",
    "* Airflow user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462ed6e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Directed Acyclic Graphs (DAGs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769889f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**DAGs** are a special subset of graphs in which the **edges** between **nodes** have a specific direction, and no **cycles** exist. When we say `no cycles exist` what we mean is the nodes cant create a path back to themselves.\n",
    "\n",
    "<img src='imgs/DAG.png' alt='DAG' width=40%>\n",
    "\n",
    "### Nodes\n",
    "A step or task in the data pipeline process.\n",
    "\n",
    "### Edges\n",
    "The dependencies or relationships other between nodes.\n",
    "\n",
    "<img src='imgs/dag_pipeline.png' alt='dag_pipeline' width=60%>\n",
    "\n",
    "\n",
    "A DAG is a collection of Tasks that are ordered to reflect the functionality, requirements and dependencies of the workflow.\n",
    "\n",
    "\n",
    "#### Are there real world cases where a data pipeline is not DAG?\n",
    "\n",
    "It is possible to model a data pipeline that is not a DAG, meaning that it contains a cycle within the process. However, the vast majority of use cases for data pipelines can be described as a directed acyclic graph (DAG). This makes the code more understandable and maintainable.\n",
    "\n",
    "#### Can we have two different pipelines for the same data and can we merge them back together?\n",
    "\n",
    "Yes. It's common for a data pipeline to take the same dataset, perform two different processes to analyze the it, then merge the results of those two processes back together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d40a02",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Components of Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f022aac3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='imgs/airflow_main_components.png' alt='airflow_main_components' width=50%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ed80d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f9371",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Scheduler** orchestrates the execution of jobs on a trigger or schedule interval. The Scheduler chooses how to prioritize the running and execution of tasks within the system. \n",
    "\n",
    "The scheduler is responsible for monitoring all DAGs and the tasks within them. When dependencies for a task are met, the scheduler triggers the task. Under the hood, the scheduler periodically inspects active tasks to trigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff75bf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b2937",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Executor**, also known as **Work Queue** is used by the scheduler in most Airflow installations to deliver tasks that need to be run to the Workers. Executors are the `workstation for tasks` and acts as a middle man to handle resource allocation and distribute task completion. \n",
    "\n",
    "There are many options available in Airflow for executors:\n",
    "* **Sequential Executor** is the default executor that runs a single task at any given time and is incapable of running tasks in parallel. It is useful for a test environment or when debugging deeper Airflow bugs.\n",
    "\n",
    "\n",
    "* The **LocalExecutor** (Single Node Arch) supports parallelism and hyperthreading and is a good fit for running Airflow on a local machine or a single node.\n",
    "\n",
    "\n",
    "* The **CeleryExecutor** is the preferred method to run a distributed Airflow cluster. It requires Redis, RabbitMq, or another message queue system to coordinate tasks between workers.\n",
    "\n",
    "\n",
    "* The **KubernetesExecutor** calls the Kubernetes API to create a temporary pod for each task instance to run. Users can pass in custom configurations for each of their tasks.\n",
    "\n",
    "\n",
    "* Debug Executor\n",
    "\n",
    "\n",
    "* Dask Executor\n",
    "\n",
    "\n",
    "* Scaling Out with Mesos (community contributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039bf5d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5704f6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The state of the DAGs and their constituent tasks needs to be saved in a **database** so that the scheduler remembers metadata information, such as the last run of a task, and the web server can retrieve this information for an end user. Airflow uses SQLAlchemy and Object Relational Mapping (ORM), written in Python, to connect to the **metadata database**. Any database supported by SQLAlchemy can be used to store all the Airflow metadata. \n",
    "\n",
    "Configurations, connections, credentials, user information, roles, policies, history, and even key-value pair variables are stored in the metadata database. The scheduler parses all the DAGs and stores the state of all tasks in the system. It also stores the relevant metadata, such as schedule intervals, statistics from each run, and their task instances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898bc69",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Web Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a1556",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Web Interface** provides a control dashboard for users and maintainers. The web interface allows users to perform tasks such as stopping and starting DAGs, retrying failed tasks, configuring credentials, The web interface visualizes the DAGs parsed by the scheduler and is built using the Flask web-development microframework. \n",
    "\n",
    "It displays the status of the jobs and allows the user to interact with the databases as well as read log files from a remote file store, such as S3, Google Cloud Storage, Azure blobs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda4c3e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d9846",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Worker** processes execute the operations defined in each DAG. When work in the queue arrives, the worker will begin to process it.\n",
    "\n",
    "In most Airflow installations, workers pull from the work queue when it is ready to process a task. When the worker completes the execution of the task, it will attempt to process more work from the work queue until there is no further work remaining. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f190016",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Order of Operations for an Airflow DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04d2cf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='imgs/af_working.png' alt='af_working' width=55%>\n",
    "\n",
    "\n",
    "* Airflow parses all the DAGs in the background at a specific period. The default period is set using the `processor_poll_interval` config, which is, by default, equal to *one second*.\n",
    "\n",
    "\n",
    "* Once a DAG file is parsed, DAG runs are created based on the scheduling parameters. Task instances are instantiated for tasks that need to be executed, and their status is set to **SCHEDULED** in the metadata database. \n",
    "\n",
    "\n",
    "> ⚠️ Since the parsing takes place periodically, any top-level code, i.e., code written in global scope in a DAG file, will execute when the scheduler parses it. This slows down the scheduler’s DAG parsing, resulting in increased usage of memory and CPU. Therefore, caution is recommended when writing code in the global scope.\n",
    "\n",
    "\n",
    "* The Airflow Scheduler starts DAGs based on time or external triggers. The scheduler is responsible for querying the database, retrieving the tasks in the **SCHEDULED** state, and distributing them to the executors. \n",
    "\n",
    "\n",
    "* Once a DAG is started, the Scheduler looks at the steps within the DAG and determines which steps can run by looking at their dependencies. The Scheduler places runnable steps in the queue, and the state for the task is changed to **QUEUED**.\n",
    "\n",
    "\n",
    "* The QUEUED tasks are drained from the queue by the workers and executed. Workers pick up those tasks and run them, thus the task status is changed to **RUNNING**.\n",
    "\n",
    "\n",
    "* Tasks get transitioned from one state to another when DAG is run. The below diagram explains the transition:\n",
    "\n",
    "<img src='imgs/transition.png' alt='transition' width=60%>\n",
    "\n",
    "\n",
    "* When a task finishes, the worker running it marks it either **failed** or **finished**. The scheduler then updates the final status in the metadata database.\n",
    "\n",
    "\n",
    "* Once all tasks have been completed, the DAG is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849a3f3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### airflow.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1e8bd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Airflow comes with lots of knobs and levers that can be tweaked to extract the desired performance from an Airflow cluster. For instance: \n",
    "* the `processor_poll_interval` config value can change the frequency which Airflow uses to parse all the DAGs in the background. \n",
    "\n",
    "\n",
    "* Another config value, `scheduler_heartbeat_sec`, controls how frequently the Airflow scheduler should attempt to look for new tasks to run. If set to fewer heartbeat seconds, the Airflow scheduler will check more frequently to trigger any new tasks, placing more pressure on the metadata database. \n",
    "\n",
    "\n",
    "* Yet another config, `job_heartbeat_sec`, determines the frequency with which task instances listen for external kill signals, e.g., when using the CLI or the UI to clear a task.\n",
    "\n",
    "\n",
    "* Other configuration reference: https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbe8d8",
   "metadata": {},
   "source": [
    "# Building a Data Pipeline in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee70c4",
   "metadata": {},
   "source": [
    "A DAG.py file is created in the DAG folder in Airflow, containing the imports for operators, DAG configurations like schedule and DAG name, and defining the dependency and sequence of tasks.\n",
    "\n",
    "<img src='imgs/operator.png' alt='operator' width=20%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9a37a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Operators and Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf916e7f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Operators** define the atomic steps of work that make up a DAG. Instantiated operators are referred to as **Tasks**.\n",
    "\n",
    "* Operators are created in the Operator folder in Airflow. They contain Python Classes that have logic to perform tasks. They are called in the DAG.py file.\n",
    "\n",
    "\n",
    "*  All operators are derived from `BaseOperator` and acquire much of their functionality through inheritance. Airflow comes with many Operators that can perform common operations:\n",
    "    * PythonOperator - calls a python function\n",
    "    * PostgresOperator - - executes a SQL command\n",
    "    * RedshiftToS3Operator\n",
    "    * S3ToRedshiftOperator\n",
    "    * BashOperator - executes a UNIX command\n",
    "    * SimpleHttpOperator\n",
    "    * Sensor - waits for a certain time, file, database row, S3 key, etc.\n",
    "    * EmailOperator - sends an email\n",
    "    \n",
    " \n",
    "* There are three main types of operators:\n",
    "    * Operators that perform an action or request another system to perform an action.\n",
    "    * Operators that transfer data from one system to another.\n",
    "    * Operators that run until a certain condition or criteria are met, e.g., a particular file lands in HDFS or S3, a Hive partition gets created, or a specific time of the day is reached. These special kinds of operators are also known as **sensors** and can allow part of our DAG to wait on some external system. All sensor operators derive from the `BaseSensorOperator` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d28ca4e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A **task** is an instantiation of an **operator** and can be thought of as a unit of work and is represented as a **node** in a DAG. A task can be as trivial as executing a bash date command or as complex as running a remote job on a Hadoop cluster.\n",
    "\n",
    "A **task instance** represents an actual run of a task. Task instances belong to DAG runs, have an associated execution_date, and are instantiable, runnable entities. Task instances go through various states, such as `running`, `success`, `failed`, `skipped`, `retry`, etc. Each task instance (and task) has a life cycle through which it moves from one state to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e56e2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Schedules "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39bb44",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A DAG, when executed, is called a **DAG run**. If we have a DAG that is scheduled to run every hour, then each instantiation of the DAG constitutes a DAG run. There could be multiple DAG runs associated with a DAG running at the same time.\n",
    "\n",
    "\n",
    "* `schedule_interval` tells when to run this dag. **Schedules** are optional, and may be defined with **cron strings** or **Airflow Presets**. Airflow provides the following presets:\n",
    "    * @once - Run a DAG once and then never again\n",
    "    * @hourly - Run the DAG every hour\n",
    "    * @daily - Run the DAG every day\n",
    "    * @weekly - Run the DAG every week\n",
    "    * @monthly - Run the DAG every month\n",
    "    * @yearly- Run the DAG every year\n",
    "    * None - Only run the DAG when the user initiates it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b7a0af",
   "metadata": {},
   "source": [
    "### Parameters/Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ffe284",
   "metadata": {},
   "source": [
    "* While creating a DAG, give it a **name**, a **description**, a **start date**, and an **interval**.\n",
    "    \n",
    "    \n",
    "* `start_date`: If start date is in the past, Airflow will run DAG as many times as there are schedule intervals between that start date and the current date.\n",
    "\n",
    "\n",
    "* `end_date`: Unless we specify an optional end date, Airflow will continue to run our DAGs until we disable or delete the DAG.\n",
    "\n",
    "\n",
    "* `max_active_run` tells how many instances of the Dag can run concurrently.\n",
    "\n",
    "\n",
    "* `retries` argument re-run a failed task multiple times before aborting the workflow run.\n",
    "\n",
    "\n",
    "* `on_success_callback` and `on_failure_callback` arguments are used to trigger some actions once the workflow succeeds or fails respectively. This will be useful to send personalized alerts to internal team via Slack, Email, or any other API call when a workflow task succeeds or fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d966a18",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### First DAG (Basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f5ce4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Initializing the default arguments that we'll pass to our DAG\n",
    "default_args = {\n",
    "    'owner': 'jamwine',\n",
    "    'start_date': datetime(2021, 1, 1),\n",
    "    'retries': 1, \n",
    "    'on_failure_callback': task_failure_alert,\n",
    "    'max_active_runs': 3\n",
    "}\n",
    "\n",
    "def message():\n",
    "    logging.info(\"Airflow is awesome!\")\n",
    "    logging.info(\"Airflow uses DAGs...\")\n",
    "\n",
    "# Creating a DAG instance\n",
    "dag1 = DAG(\n",
    "        dag_id=\"dag1\",\n",
    "        description=\"Test DAG 1\",\n",
    "        default_args=default_args,\n",
    "        start_date=datetime.now(),\n",
    "        schedule_interval=\"@hourly\"\n",
    ")\n",
    "\n",
    "message_task = PythonOperator(\n",
    "    task_id=\"message_task\",\n",
    "    python_callable=message,\n",
    "    dag=dag1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fa9d0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28dd55",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Task Dependencies** can be described programmatically in Airflow using `>>` and `<<`\n",
    "* `a >> b` means **a comes before b**\n",
    "* `a << b` means **a comes after b**\n",
    "\n",
    "Tasks dependencies can also be set with `set_downstream` and `set_upstream`.\n",
    "* `a.set_downstream(b)` means **a comes before b**\n",
    "* `a.set_upstream(b)` means **a comes after b**\n",
    "\n",
    "Parallel tasks are included in `[]`. For example, `[a,b] >> c`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566ca1fb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005d76d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **Variables** are defined as a generic way to store and retrieve arbitrary content within Airflow. They are represented as a simple key value stored into the meta database of Airflow.\n",
    "\n",
    "\n",
    "*  Variables are useful for storing and retrieving data at runtime, in avoiding hard-coding values, and from code repetitions within our DAGs.\n",
    "\n",
    "\n",
    "* Airflow goes through **2-layers** before reaching the metastore. If the variable is found in one of these two layers, Airflow doesn’t need to create a connection, thus it is better optimized.\n",
    "\n",
    "\n",
    "*  Command Line Interface and Environment Variables Reference: https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfee978",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### XComs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7f5b4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **XComs** or **Cross Communication messages** is designed to communicate small amount of data between tasks. Basically, they let tasks exchange messages within our DAG. For example, if task_B depends on task_A, task_A can push data into a XCom and task_B can pull this data to use it. \n",
    "\n",
    "\n",
    "* We use `xcom_push` and `xcom_pull` to push and retrieve variables.\n",
    "\n",
    "\n",
    "* These methods `xcom_push` and `xcom_pull` are only accessible from a task instance object. With the PythonOperator, we can access them by passing the parameter `ti` to the python callable function. \n",
    "\n",
    "\n",
    "* We can pull XComs from multiple tasks at once.\n",
    "\n",
    "\n",
    "* In the BashOperator, `do_xcom_push` allows us to push the **last line** written to stdout into a XCom. By default, `do_xcom_push` is set to **True**.\n",
    "\n",
    "\n",
    "*  By default, when a XCom is automatically created by returning a value, Airflow assigns the key `return_value`. The key `return_value` indicates that this XCom has been created by returning the value from the operator. The XCom values gets stored into the `metadata database` of Airflow with the key `return_value`.\n",
    "\n",
    "<img src='imgs/af_variable.png' alt='af_variable' width=40%>\n",
    "\n",
    "* XComs create implicit dependencies between the tasks that are not visible from the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a69b9f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Macros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eba2d0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **Jinja templates** and **Macros** in Apache Airflow are the way to pass dynamic data to our DAGs at runtime. \n",
    "\n",
    "\n",
    "* Templating allows us to interpolate values at run time in static files such as HTML or SQL files, by placing special placeholders in them indicating where the values should be and/or how they should be displayed.\n",
    "\n",
    "\n",
    "* The curly brackets `{{ }}` represent **placeholders** in which the value is replaced at the **runtime** each time the DAG gets triggered. For example, `{{ ds }}` is a macro that gets replaced by the **execution date** of the DAG.\n",
    "\n",
    "\n",
    "* **Macros** are functions that take an input, modify that input and give the modified output. Macros can be used in our templates by calling them with the following notation: `macro.macro_func()`.\n",
    "\n",
    "\n",
    "* Apache Airflow brings predefined variables that we can use in our templates. They are very useful since they allow us to have information about the current executing DAG and task. Macros Reference: https://airflow.apache.org/docs/apache-airflow/stable/macros-ref.html\n",
    "\n",
    "\n",
    "*  We can also check our Jinja Templates rendered before even executing our DAG in the Rendered section of Airflow UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a006e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55a05e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* https://livebook.manning.com/book/data-pipelines-with-apache-airflow/chapter-1/v-5/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c2b56",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
